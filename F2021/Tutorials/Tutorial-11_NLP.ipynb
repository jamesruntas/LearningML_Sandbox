{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tutorial-11-NLP.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFZIKVNfjDcZ"
      },
      "source": [
        "# Tutorial 11 - Natural Language Processing\n",
        "\n",
        "**Semester:** Fall 2021\n",
        "\n",
        "**Adapted by:** [Kevin Dick](https://kevindick.ai/)\n",
        "\n",
        "**PART I Concepts adapted from:** [Ventsislav Yordanov's](https://medium.com/@ventsislav94) [Introduction to Natural Language Processing for Text](https://towardsdatascience.com/introduction-to-natural-language-processing-for-text-df845750fb63).\n",
        "\n",
        "**PART II Notebooks adapted from:** [HuggingFace's Transformer Notebooks](https://huggingface.co/transformers/notebooks.html) particularly the [Getting Started with Transformers Notebook](https://github.com/huggingface/transformers/blob/master/notebooks/02-transformers.ipynb)\n",
        "\n",
        "**PART III Notebooks adapted from:** [José Eduardo Storopoli](https://github.com/storopoli)'s [Topic Modelling Notebooks](https://github.com/storopoli/topic-modelling/tree/master/Notebooks)\n",
        "\n",
        "---\n",
        "\n",
        "**Tangential Aside**: For anyone planning on pursuing advanced research in the field of NLP, I strongly suggest familiarizing yourselves with [Zipf's law](https://en.wikipedia.org/wiki/Zipf%27s_law) which is a universal distribution of word frequencies within anyy (and all!) languages: [Fantastic (& Quizacious ;) VSauce Video Disccussing the Zipf Mystery](https://youtu.be/fCn8zs912OE)\n",
        "\n",
        "---\n",
        "\n",
        "### PART I: What is Natural Language Processing (NLP)?\n",
        "\n",
        "NLP is a **subfield of machine learning** concerned with the application of **learning algorthms to text and speech**. More generally, NLP-based methods are typically applicable to all sequential-type information (*e.g.* DNA sequences, audio signals, time-series signals, *etc.*) however, they are predominantly used in human language applications.\n",
        "\n",
        "For example, we can use NLP to create systems including:\n",
        "1. **speech recognition** (*e.g.* real-time captioning)\n",
        "2. **document summarization** \n",
        "3. **machine translation**\n",
        "4. **spam detection**\n",
        "5. **named entity recognition**\n",
        "6. **question answering**\n",
        "7. **autocomplete** (*i.e.* predictive typing)\n",
        "\n",
        "**General Data Processing Pipeline:** In order to tackle each of these tasks, we must first process text into a format amenable for use by NLP learning algorithms.\n",
        "\n",
        "### The NLTK Library for the Basics of NLP [(See Details Here)](https://towardsdatascience.com/introduction-to-natural-language-processing-for-text-df845750fb63)\n",
        "NLTK (Natural Language Toolkit) is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to many corpora (large document collections) and lexical resources. Also, it contains a suite of text processing libraries for **classification**, **tokenization**, **stemming**, **tagging**, **parsing**, and **semantic reasoning**. Best of all, NLTK is a free, open source, community-driven project.\n",
        "\n",
        "1. **Sentence Tokenization:** Sentence tokenization (also called **sentence segmentation**) is the problem of *dividing a string of written language into its component sentences*.\n",
        "2. **Word Tokenization**: Word tokenization (also called **word segmentation**) is the problem of *dividing a string of written language into its component words*.\n",
        "3. **Text Lemmatization & Stemming**: The goal of both stemming (crude) and lemmatization (refiined) is to *reduce inflectional forms* and sometimes derivationally related forms of a word to a common base form. For example, \"drive\" & \"drives\" & \"driving\" all have the same semantic meaning and should be combined.\n",
        "4. **Stop Words**: Stop words usually refer to the **most common words** such as “and”, “the”, “a” in a language and when applying machine learning to text, **these words can add a lot of noise** so we remove them.\n",
        "5. **Regex**: A regular expression, regex, or regexp is a sequence of characters that define *a search pattern to apply additional filtering* to our text. For example, we can remove all the non-words characters. In many cases, we don’t need the punctuation marks and it’s easy to remove them with regex.\n",
        "6. **Bag-of-Words**: Machine learning algorithms *cannot work with raw text directly*, we need to convert the text into vectors of numbers (i.e. feature extraction) and the *bag-of-words mode*l is a popular and simple feature extraction technique that *counts the occurrence of each word within a document*.\n",
        "7. **TF-IDF**: One problem with scoring word frequency is that *the most frequent words in the document start to have the highest scores* (frequent words may not havee much “informational gain”) to the model so we penalize words that are frequent across all the documents using TF-IDF (**term frequency-inverse document frequency** is a s**tatistical measure** used to evaluate the **importance of a word** to a **document in a collection or corpus**).\n",
        "\n",
        "---\n",
        "\n",
        "### Demonstration in Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oxTz9BeY0JvQ",
        "outputId": "10ff0687-a5a3-43a9-9d76-9b848c4666ed"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGjV8O2V0NRS"
      },
      "source": [
        " ### 1. Sentence Tokenization\n",
        " Split the text of string (from a document) into individuual sentences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W_xAVCQhiqUi",
        "outputId": "24c8b206-584e-4510-8bed-22897ed0f7ef"
      },
      "source": [
        "# For visual conveenience, this is represented as a string block, but it should\n",
        "# be conceptualized as a single long and continuous string.\n",
        "text = \"\"\"Backgammon is one of the oldest known board games. \n",
        "          Its history can be traced back nearly 5,000 years to archeological discoveries in the Middle East. \n",
        "          It is a two player game where each player has fifteen checkers which move between twenty-four points according to the roll of two dice.\n",
        "       \"\"\"\n",
        "\n",
        "sentences = nltk.sent_tokenize(text)\n",
        "for i, sentence in enumerate(sentences):\n",
        "    print(f'{i}: \"{sentence}\"')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0: \"Backgammon is one of the oldest known board games.\"\n",
            "1: \"Its history can be traced back nearly 5,000 years to archeological discoveries in the Middle East.\"\n",
            "2: \"It is a two player game where each player has fifteen checkers which move between twenty-four points according to the roll of two dice.\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWVt6R7z0ljY"
      },
      "source": [
        "### 2. Word Tokenization\n",
        "Split each sentence into individual words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kvwt81M30rJa",
        "outputId": "ce391ce6-51d8-4ae9-d722-646d1f5019eb"
      },
      "source": [
        "for i, sentence in enumerate(sentences):\n",
        "    words = nltk.word_tokenize(sentence)\n",
        "    print(f'{i}: {words}')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0: ['Backgammon', 'is', 'one', 'of', 'the', 'oldest', 'known', 'board', 'games', '.']\n",
            "1: ['Its', 'history', 'can', 'be', 'traced', 'back', 'nearly', '5,000', 'years', 'to', 'archeological', 'discoveries', 'in', 'the', 'Middle', 'East', '.']\n",
            "2: ['It', 'is', 'a', 'two', 'player', 'game', 'where', 'each', 'player', 'has', 'fifteen', 'checkers', 'which', 'move', 'between', 'twenty-four', 'points', 'according', 'to', 'the', 'roll', 'of', 'two', 'dice', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1_IDm5g1Xyx"
      },
      "source": [
        "### 3. Text Lemmatization & Stemming\n",
        "Reduce the semantic-space by reducing smilar words to a common semanttic token."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zoZzMIpj1v1z",
        "outputId": "96ff6d04-564b-4266-86c5-37a9b9dd8649"
      },
      "source": [
        "nltk.download('wordnet')\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "def compare_stemmer_and_lemmatizer(stemmer, lemmatizer, word, pos):\n",
        "    \"\"\"\n",
        "    Print the results of stemming and lemmitization using the passed stemmer, \n",
        "    lemmatizer, word and pos (part of speech)\n",
        "    \"\"\"\n",
        "    print(f'Input Word: {word}\\n{\"-\" * 10}')\n",
        "    print(f'Stemmer: {stemmer.stem(word)}\\nLemmatizer: {lemmatizer.lemmatize(word, pos)}')\n",
        "    print('=' * 15)\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stemmer = PorterStemmer()\n",
        "compare_stemmer_and_lemmatizer(stemmer, lemmatizer, word = \"seen\", pos = wordnet.VERB)\n",
        "compare_stemmer_and_lemmatizer(stemmer, lemmatizer, word = \"drove\", pos = wordnet.VERB)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "Input Word: seen\n",
            "----------\n",
            "Stemmer: seen\n",
            "Lemmatizer: see\n",
            "===============\n",
            "Input Word: drove\n",
            "----------\n",
            "Stemmer: drove\n",
            "Lemmatizer: drive\n",
            "===============\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLaecDM63Wj4"
      },
      "source": [
        "### 4. Stop Words\n",
        "Remove high-frequency and \"noisy\" words that don't add to the sentence meaning."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IBBmM7dn3hgM",
        "outputId": "3045133e-4f8c-4115-850a-8e1936acdb04"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "print(stopwords.words(\"english\"))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzcXRWvt3yoz"
      },
      "source": [
        "Let's remove the stop-words from a sentence:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uZ7A9o6K3iFN",
        "outputId": "61eb0b2a-fb3b-4edf-9b66-c2a2fdc1182c"
      },
      "source": [
        "stop_words = set(stopwords.words(\"english\"))\n",
        "sentence = \"Backgammon is one of the oldest known board games.\"\n",
        "\n",
        "words = nltk.word_tokenize(sentence)\n",
        "without_stop_words = [word for word in words if not word in stop_words]\n",
        "print(without_stop_words)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Backgammon', 'one', 'oldest', 'known', 'board', 'games', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvuHOJWI5lTp"
      },
      "source": [
        "### 5. Regular Expression Filtering\n",
        "Regex are useful for filtering text based on specific patterns.\n",
        "\n",
        "Here, we remove all punctuation (remove anything that doesnt match a word)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nt0a5BIy37Ez",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e800bf3f-6fd5-4a85-d313-7a3c785b7a89"
      },
      "source": [
        "import re\n",
        "sentence = \"The development of snowboarding was inspired by skateboarding, sledding, surfing and skiing.\"\n",
        "pattern = r\"[^\\w]\" # Translates to: NOT MATCH WORD\n",
        "print(re.sub(pattern, \" \", sentence))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The development of snowboarding was inspired by skateboarding  sledding  surfing and skiing \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_UdXTKB6XO6"
      },
      "source": [
        "* . - match any character except newline\n",
        "* \\w - match word\n",
        "* \\d - match digit\n",
        "* \\s - match whitespace\n",
        "* \\W - match not word\n",
        "* \\D - match not digit\n",
        "* \\S - match not whitespace\n",
        "* [abc] - match any of a, b, or c\n",
        "* [^abc] - not match a, b, or c\n",
        "* [a-g] - match a character between a & g\n",
        "\n",
        "### 6. Bag-of-Words\n",
        "\n",
        "This is a limited example of a Bag-of-Words application."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "id": "v3R71oCg6hOj",
        "outputId": "8137378b-6804-4371-f251-b9e8ade01191"
      },
      "source": [
        "# Import the libraries we need\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import pandas as pd\n",
        "\n",
        "# Normally you would load this from a file.\n",
        "raw_text = \"\"\"I like this movie, it's funny. \n",
        "              I hate this movie.\n",
        "              This was awesome! I like it.\n",
        "              Nice one. I love it.\"\"\"\n",
        "\n",
        "# Step 1. Design the Vocabulary\n",
        "#   The default token pattern removes tokens of a single character. \n",
        "#   That's why we don't have the \"I\" and \"s\" tokens in the output\n",
        "count_vectorizer = CountVectorizer()\n",
        "\n",
        "# Step 2. Create the Bag-of-Words Model\n",
        "bag_of_words = count_vectorizer.fit_transform(raw_text.splitlines())\n",
        "\n",
        "# Show the Bag-of-Words Model as a Pandas DataFrame\n",
        "#   NOTE: the sum of columns generates a \"concordance\" (SYSC 1005 Easter Egg!)\n",
        "feature_names = count_vectorizer.get_feature_names()\n",
        "pd.DataFrame(bag_of_words.toarray(), columns = feature_names)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>awesome</th>\n",
              "      <th>funny</th>\n",
              "      <th>hate</th>\n",
              "      <th>it</th>\n",
              "      <th>like</th>\n",
              "      <th>love</th>\n",
              "      <th>movie</th>\n",
              "      <th>nice</th>\n",
              "      <th>one</th>\n",
              "      <th>this</th>\n",
              "      <th>was</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   awesome  funny  hate  it  like  love  movie  nice  one  this  was\n",
              "0        0      1     0   1     1     0      1     0    0     1    0\n",
              "1        0      0     1   0     0     0      1     0    0     1    0\n",
              "2        1      0     0   1     1     0      0     0    0     1    1\n",
              "3        0      0     0   1     0     1      0     1    1     0    0"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltqt5qW17IHu"
      },
      "source": [
        "Considering a large amount of data in most big data applications, the length of the vector that represents a document might be **thousands or millions of elements.** Furthermore, each document may contain only a few of the known words in the vocabulary.\n",
        "\n",
        "Therefore the **vector representations will have a lot of zeros** and will therefore be **sparse vectors** that ttypically require **more memory and computational resources**.\n",
        "\n",
        "#### Vocabulary Simplification using *n*-grams: \n",
        "A more complex way to create a vocabulary is to use **grouped words**. This **changes the scope of the vocabulary** and allows the bag-of-words model to get more details about the document.\n",
        "\n",
        "### 7. TF-IDF: Term Frequency-Inverse Document Frequency\n",
        "The TF-IDF scoring value increases proportionally to the number of times a word appears in the document, but it is offset by the number of documents in the corpus that contain the word.\n",
        "\n",
        "![](https://miro.medium.com/max/700/1*V9ac4hLVyms79jl65Ym_Bw.png)\n",
        "\n",
        "**Term Frequency (TF)**: a scoring of the frequency of the word in the current document.\n",
        "\n",
        "![](https://miro.medium.com/max/463/1*V3qfsHl0t-bV5kA0mlnsjQ.png)\n",
        "\n",
        "**Inverse Document Frequency (IDF)**: a scoring of how rare the word is across documents.\n",
        "\n",
        "![](https://miro.medium.com/max/445/1*wvPGL02y36QL7-tdG1BT1A.png)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "id": "fjzy7V3t-xve",
        "outputId": "6f64e998-3bd0-44d4-94b5-c2606e9827b4"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd\n",
        "\n",
        "# Normally you would load this from a file.\n",
        "raw_text = \"\"\"I like this movie, it's funny. \n",
        "              I hate this movie.\n",
        "              This was awesome! I like it.\n",
        "              Nice one. I love it.\"\"\"\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "values = tfidf_vectorizer.fit_transform(raw_text.splitlines())\n",
        "\n",
        "# Show the Model as a pandas DataFrame\n",
        "feature_names = tfidf_vectorizer.get_feature_names()\n",
        "pd.DataFrame(values.toarray(), columns = feature_names)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>awesome</th>\n",
              "      <th>funny</th>\n",
              "      <th>hate</th>\n",
              "      <th>it</th>\n",
              "      <th>like</th>\n",
              "      <th>love</th>\n",
              "      <th>movie</th>\n",
              "      <th>nice</th>\n",
              "      <th>one</th>\n",
              "      <th>this</th>\n",
              "      <th>was</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.571848</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.365003</td>\n",
              "      <td>0.450852</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.450852</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.365003</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.702035</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.553492</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.448100</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.539445</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.344321</td>\n",
              "      <td>0.425305</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.344321</td>\n",
              "      <td>0.539445</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.345783</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.541736</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.541736</td>\n",
              "      <td>0.541736</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    awesome     funny      hate  ...       one      this       was\n",
              "0  0.000000  0.571848  0.000000  ...  0.000000  0.365003  0.000000\n",
              "1  0.000000  0.000000  0.702035  ...  0.000000  0.448100  0.000000\n",
              "2  0.539445  0.000000  0.000000  ...  0.000000  0.344321  0.539445\n",
              "3  0.000000  0.000000  0.000000  ...  0.541736  0.000000  0.000000\n",
              "\n",
              "[4 rows x 11 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cfP1GKWBFVK"
      },
      "source": [
        "---\n",
        "\n",
        "## PART II: Introduction to NLP Transformer-based Methods (NOTE: Time Consuming)\n",
        "The transformers library is an open-source, community-based repository to train, use and share models based on \n",
        "the Transformer architecture [(Vaswani & al., 2017)](https://arxiv.org/abs/1706.03762) such as Bert [(Devlin & al., 2018)](https://arxiv.org/abs/1810.04805),\n",
        "Roberta [(Liu & al., 2019)](https://arxiv.org/abs/1907.11692), GPT2 [(Radford & al., 2019)](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf),\n",
        "XLNet [(Yang & al., 2019)](https://arxiv.org/abs/1906.08237), etc. \n",
        "\n",
        "Along with the models, the library contains multiple variations of each of them for a large variety of \n",
        "downstream-tasks like **Named Entity Recognition (NER)**, **Sentiment Analysis**, \n",
        "**Language Modeling**, **Question Answering** and so on.\n",
        "\n",
        "## The Recurrent Neural Networks that Preceded the Transformer\n",
        "\n",
        "In 2017, most Neural Network application to Natural Language Processing relied on the sequential processing of the input through [Recurrent Neural Network (RNN)](https://en.wikipedia.org/wiki/Recurrent_neural_network).\n",
        "\n",
        "![rnn](http://colah.github.io/posts/2015-09-NN-Types-FP/img/RNN-general.png)   \n",
        "\n",
        "RNNs performed well for a large variety of tasks involving sequential dependenciies over the input sequence, however, the sequentially-dependent process had issues modeling very long range dependencies and was not well suited for the kind of hardware we're currently leveraging (poor ability to parallelize computation). \n",
        "\n",
        "Most recently, the Attention mechanism was introduced as an improvement over \"raw\" RNNs by giving  a learned, weighted-importance to each element in the sequence, allowing the model to focus on \"important\" elements.\n",
        "\n",
        "![attention_rnn](https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2017/08/Example-of-Attention.png)  \n",
        "\n",
        "## Then Came the Transformer  \n",
        "\n",
        "Then in 2017, [(Vaswani & al., 2017)](https://arxiv.org/abs/1706.03762)\n",
        "heralded the Transformer-era by demonstrating superiority over [Recurrent Neural Network (RNN)](https://en.wikipedia.org/wiki/Recurrent_neural_network)\n",
        "on translation tasks and Attention-based methods were quickly extended to almost all RNN-type tasks overcoming the State-of-the-Art at the time.\n",
        "\n",
        "One advantage of the Transformer architechtture over its RNN counterpart is its non-sequential attention model. Recall that RNNs have to iterate over each element of the input sequence one-by-one and carry an \"updatable-state\" between each hop. Conversely, Transformer models are able to look at every position in the sequence, at the same time, in one operation converting a formerly serial-type task into an embarassingly paarallel one.\n",
        "\n",
        "Read [The Annotated Transformer](https://nlp.seas.harvard.edu/2018/04/03/attention.html#encoder-and-decoder-stacks) for a deep-dive into the Transformer architecture.\n",
        "\n",
        "![transformer-encoder-decoder](https://nlp.seas.harvard.edu/images/the-annotated-transformer_14_0.png)\n",
        "\n",
        "## Getting started with transformers\n",
        "\n",
        "For the rest of this notebook, we will use the [BERT (Devlin & al., 2018)](https://arxiv.org/abs/1810.04805) architecture, as it's the most simple and there are plenty of content about it over the internet (it will be easy to dig more over this architecture if you want to).\n",
        "\n",
        "The transformers library allows you to benefits from large, pretrained language models without requiring a huge and costly computational\n",
        "infrastructure. Most of the State-of-the-Art models are provided directly by their author and made available in the library \n",
        "in PyTorch and TensorFlow in a transparent and interchangeable way. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QP6uSHWMQNjG"
      },
      "source": [
        "# How to train a new language model from scratch using Transformers and Tokenizers\n",
        "\n",
        "Over the past few months, we made several improvements to our [`transformers`](https://github.com/huggingface/transformers) and [`tokenizers`](https://github.com/huggingface/tokenizers) libraries, with the goal of making it easier than ever to **train a new language model from scratch**.\n",
        "\n",
        "In this post we’ll demo how to train a “small” model (84 M parameters = 6 layers, 768 hidden size, 12 attention heads) – that’s the same number of layers & heads as DistilBERT – on **Esperanto**. We’ll then fine-tune the model on a downstream task of part-of-speech tagging.\n",
        "\n",
        "\n",
        "## 1. Find a dataset\n",
        "\n",
        "First, let us find a corpus of text in Esperanto. Here we’ll use the Esperanto portion of the [OSCAR corpus](https://traces1.inria.fr/oscar/) from INRIA.\n",
        "OSCAR is a huge multilingual corpus obtained by language classification and filtering of [Common Crawl](https://commoncrawl.org/) dumps of the Web.\n",
        "\n",
        "<img src=\"https://huggingface.co/blog/assets/01_how-to-train/oscar.png\" style=\"margin: auto; display: block; width: 260px;\">\n",
        "\n",
        "The Esperanto portion of the dataset is only 299M, so we’ll concatenate with the Esperanto sub-corpus of the [Leipzig Corpora Collection](https://wortschatz.uni-leipzig.de/en/download), which is comprised of text from diverse sources like news, literature, and wikipedia.\n",
        "\n",
        "The final training corpus has a size of 3 GB, which is still small – for your model, you will get better results the more data you can get to pretrain on. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NLeODKeoQNDo",
        "outputId": "e8964c8f-3217-4a1e-f76d-aa640caab66c"
      },
      "source": [
        "# in this notebook we'll only get one of the files (the Oscar one) for the sake of simplicity and performance\n",
        "!wget -c https://cdn-datasets.huggingface.co/EsperBERTo/data/oscar.eo.txt"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-12-05 18:04:49--  https://cdn-datasets.huggingface.co/EsperBERTo/data/oscar.eo.txt\n",
            "Resolving cdn-datasets.huggingface.co (cdn-datasets.huggingface.co)... 54.192.18.43, 54.192.18.17, 54.192.18.90, ...\n",
            "Connecting to cdn-datasets.huggingface.co (cdn-datasets.huggingface.co)|54.192.18.43|:443... connected.\n",
            "HTTP request sent, awaiting response... 416 Requested Range Not Satisfiable\n",
            "\n",
            "    The file is already fully retrieved; nothing to do.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-vi8KNSNQZNT"
      },
      "source": [
        "## 2. Train a tokenizer\n",
        "\n",
        "We choose to train a byte-level Byte-pair encoding tokenizer (the same as GPT-2), with the same special tokens as RoBERTa. Let’s arbitrarily pick its size to be 52,000.\n",
        "\n",
        "We recommend training a byte-level BPE (rather than let’s say, a WordPiece tokenizer like BERT) because it will start building its vocabulary from an alphabet of single bytes, so all words will be decomposable into tokens (no more `<unk>` tokens!).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1q_i98H1QZ8l",
        "outputId": "37a74f03-5ec1-46dd-aa3e-8fdccce442d1"
      },
      "source": [
        "# We won't need TensorFlow here\n",
        "!pip uninstall -y tensorflow\n",
        "# Install `transformers` from master\n",
        "!pip install git+https://github.com/huggingface/transformers\n",
        "!pip list | grep -E 'transformers|tokenizers'\n",
        "# transformers version at notebook update --- 2.11.0\n",
        "# tokenizers version at notebook update --- 0.8.0rc1"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping tensorflow as it is not installed.\u001b[0m\n",
            "Collecting git+https://github.com/huggingface/transformers\n",
            "  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-ai4slu1v\n",
            "  Running command git clone -q https://github.com/huggingface/transformers /tmp/pip-req-build-ai4slu1v\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==4.13.0.dev0) (0.0.46)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.13.0.dev0) (0.2.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.13.0.dev0) (4.62.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.13.0.dev0) (4.8.2)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.13.0.dev0) (0.10.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.13.0.dev0) (21.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.13.0.dev0) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.13.0.dev0) (2.23.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.13.0.dev0) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.13.0.dev0) (3.4.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.13.0.dev0) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.13.0.dev0) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers==4.13.0.dev0) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.13.0.dev0) (3.6.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.13.0.dev0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.13.0.dev0) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.13.0.dev0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.13.0.dev0) (1.24.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.13.0.dev0) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.13.0.dev0) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.13.0.dev0) (7.1.2)\n",
            "tokenizers                    0.10.3\n",
            "transformers                  4.13.0.dev0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ChbPPd2GQjIT",
        "outputId": "794d0add-9f66-4d3f-c508-743e469f55d1"
      },
      "source": [
        "%%time \n",
        "from pathlib import Path\n",
        "\n",
        "from tokenizers import ByteLevelBPETokenizer\n",
        "\n",
        "paths = [str(x) for x in Path(\".\").glob(\"**/*.txt\")]\n",
        "\n",
        "# Initialize a tokenizer\n",
        "tokenizer = ByteLevelBPETokenizer()\n",
        "\n",
        "# Customize training\n",
        "tokenizer.train(files=paths, vocab_size=52_000, min_frequency=2, special_tokens=[\n",
        "    \"<s>\",\n",
        "    \"<pad>\",\n",
        "    \"</s>\",\n",
        "    \"<unk>\",\n",
        "    \"<mask>\",\n",
        "])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 17min 50s, sys: 4.83 s, total: 17min 54s\n",
            "Wall time: 9min 13s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V1SQChFnTG6F",
        "outputId": "e8e75a34-4505-440d-dab9-3f9bdbaaa548"
      },
      "source": [
        "!mkdir EsperBERTo\n",
        "tokenizer.save_model(\"EsperBERTo\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘EsperBERTo’: File exists\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['EsperBERTo/vocab.json', 'EsperBERTo/merges.txt']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jwaKLs1rTOny"
      },
      "source": [
        "What is great is that our tokenizer is optimized for Esperanto. Compared to a generic tokenizer trained for English, more native words are represented by a single, unsplit token. Diacritics, i.e. accented characters used in Esperanto – `ĉ`, `ĝ`, `ĥ`, `ĵ`, `ŝ`, and `ŭ` – are encoded natively. We also represent sequences in a more efficient manner. Here on this corpus, the average length of encoded sequences is ~30% smaller as when using the pretrained GPT-2 tokenizer.\n",
        "\n",
        "Here’s  how you can use it in `tokenizers`, including handling the RoBERTa special tokens – of course, you’ll also be able to use it directly from `transformers`.\n",
        "\n",
        "We now have both a `vocab.json`, which is a list of the most frequent tokens ranked by frequency, and a `merges.txt` list of merges.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JzTMGOyCTejv"
      },
      "source": [
        "from tokenizers.implementations import ByteLevelBPETokenizer\n",
        "from tokenizers.processors import BertProcessing\n",
        "\n",
        "\n",
        "tokenizer = ByteLevelBPETokenizer(\n",
        "    \"./EsperBERTo/vocab.json\",\n",
        "    \"./EsperBERTo/merges.txt\",\n",
        ")"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PP3uwf9SThy5"
      },
      "source": [
        "tokenizer._tokenizer.post_processor = BertProcessing(\n",
        "    (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
        "    (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
        ")\n",
        "tokenizer.enable_truncation(max_length=512)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ItZJfD6nTqdO",
        "outputId": "21931179-37d3-4802-e71e-31582730b1c9"
      },
      "source": [
        "tokenizer.encode(\"Mi estas Kevin.\").tokens"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<s>', 'Mi', 'Ġestas', 'ĠKe', 'vin', '.', '</s>']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HJ2_xL6V_r_"
      },
      "source": [
        "**bold text**## 3. Train a language model from scratch\n",
        "\n",
        "**Update:** This section follows along the [`run_language_modeling.py`](https://github.com/huggingface/transformers/blob/master/examples/language-modeling/run_language_modeling.py) script, using our new [`Trainer`](https://github.com/huggingface/transformers/blob/master/src/transformers/trainer.py) directly. Feel free to pick the approach you like best.\n",
        "\n",
        "> We’ll train a RoBERTa-like model, which is a BERT-like with a couple of changes (check the [documentation](https://huggingface.co/transformers/model_doc/roberta.html) for more details).\n",
        "\n",
        "As the model is BERT-like, we’ll train it on a task of *Masked language modeling*, i.e. the predict how to fill arbitrary tokens that we randomly mask in the dataset. This is taken care of by the example script.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YoiRlv-UWARS",
        "outputId": "58875e22-e1db-41ca-d45a-fc9a8228a352"
      },
      "source": [
        "# Check that we have a GPU\n",
        "!nvidia-smi"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Dec  5 18:15:10 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 495.44       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   52C    P8    29W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EUUgUOfbWHA2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "707dd1e6-b28e-47cf-c765-6e175e4c88d9"
      },
      "source": [
        "# Check that PyTorch sees it\n",
        "import torch\n",
        "torch.cuda.is_available()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJWdceEgWJUf"
      },
      "source": [
        "from transformers import RobertaConfig\n",
        "\n",
        "config = RobertaConfig(\n",
        "    vocab_size=52_000,\n",
        "    max_position_embeddings=514,\n",
        "    num_attention_heads=12,\n",
        "    num_hidden_layers=6,\n",
        "    type_vocab_size=1,\n",
        ")"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5RXlwtvjWL4d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3a7f7e8-1868-4b51-a0fe-6733cc73ba89"
      },
      "source": [
        "# Now let's re-create our tokenizer in transformers\n",
        "from transformers import RobertaTokenizerFast\n",
        "tokenizer = RobertaTokenizerFast.from_pretrained(\"./EsperBERTo\", max_len=512)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "file ./EsperBERTo/config.json not found\n",
            "file ./EsperBERTo/config.json not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7dU72l1WSEx"
      },
      "source": [
        "Finally let's initialize our model.\n",
        "\n",
        "**Important:**\n",
        "\n",
        "As we are training from scratch, we only initialize from a config, not from an existing pretrained model or checkpoint."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sEC1gmylWUES",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7971add6-9351-4324-f307-f21220a043b5"
      },
      "source": [
        "from transformers import RobertaForMaskedLM\n",
        "\n",
        "model = RobertaForMaskedLM(config=config)\n",
        "model.num_parameters()\n",
        "# => 84 million parameters"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "83504416"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HlOXDEE8WY0t"
      },
      "source": [
        "### Now let's build our training Dataset\n",
        "\n",
        "We'll build our dataset by applying our tokenizer to our text file.\n",
        "\n",
        "Here, as we only have one text file, we don't even need to customize our `Dataset`. We'll just use the `LineByLineDataset` out-of-the-box."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TxMVbP0FWa-x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "894c7c74-60f5-4b0f-a6d5-42a0e8710634"
      },
      "source": [
        "%%time\n",
        "from transformers import LineByLineTextDataset\n",
        "\n",
        "dataset = LineByLineTextDataset(\n",
        "    tokenizer=tokenizer,\n",
        "    file_path=\"./oscar.eo.txt\",\n",
        "    block_size=128,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/data/datasets/language_modeling.py:125: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/pytorch/language-modeling/run_mlm.py\n",
            "  FutureWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Rco5XoOWc_e"
      },
      "source": [
        "Like in the [`run_language_modeling.py`](https://github.com/huggingface/transformers/blob/master/examples/language-modeling/run_language_modeling.py) script, we need to define a data_collator.\n",
        "\n",
        "This is just a small helper that will help us batch different samples of the dataset together into an object that PyTorch knows how to perform backprop on."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PB9DKc3uWe-1"
      },
      "source": [
        "from transformers import DataCollatorForLanguageModeling\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "icYoXr7SWiVy"
      },
      "source": [
        "### Finally, we are all set to initialize our Trainer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8mLxWFrZWkfL"
      },
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./EsperBERTo\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=1,\n",
        "    per_gpu_train_batch_size=64,\n",
        "    save_steps=10_000,\n",
        "    save_total_limit=2,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=dataset,\n",
        "    prediction_loss_only=True,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qImPT0TLWpDk"
      },
      "source": [
        "#### Start Training (~1hr+)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ank2Pww5WoN4"
      },
      "source": [
        "%%time\n",
        "trainer.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iec9qQ4wWwBH"
      },
      "source": [
        "trainer.save_model(\"./EsperBERTo\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUjPux70W0G1"
      },
      "source": [
        "#### Check that the Language Model Actually Learned\n",
        "\n",
        "Aside from looking at the training and eval losses going down, the easiest way to check whether our language model is learning anything interesting is via the `FillMaskPipeline`.\n",
        "\n",
        "Pipelines are simple wrappers around tokenizers and models, and the 'fill-mask' one will let you input a sequence containing a masked token (here, `<mask>`) and return a list of the most probable filled sequences, with their probabilities."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5a87vBe-W9Tr"
      },
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "fill_mask = pipeline(\n",
        "    \"fill-mask\",\n",
        "    model=\"./EsperBERTo\",\n",
        "    tokenizer=\"./EsperBERTo\"\n",
        ")\n",
        "\n",
        "# The sun <mask>.\n",
        "# =>\n",
        "\n",
        "fill_mask(\"La suno <mask>.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XNVgnw8YXF8a"
      },
      "source": [
        "Ok, simple syntax/grammar works. Let’s try a slightly more interesting prompt:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O90uVhXgXE-O"
      },
      "source": [
        "fill_mask(\"Jen la komenco de bela <mask>.\")\n",
        "\n",
        "# This is the beginning of a beautiful <mask>.\n",
        "# =>"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZVBSx1RXLYd"
      },
      "source": [
        "### Share your Model with the Community\n",
        "\n",
        "Finally, when you have a nice model, please think about sharing it with the community:\n",
        "\n",
        "- upload your model using the CLI: `transformers-cli upload`\n",
        "- write a README.md model card and add it to the repository under `model_cards/`. Your model card should ideally include:\n",
        "    - a model description,\n",
        "    - training params (dataset, preprocessing, hyperparameters), \n",
        "    - evaluation results,\n",
        "    - intended uses & limitations\n",
        "    - whatever else is helpful! 🤓\n",
        "\n",
        "### **TADA!**\n",
        "\n",
        "➡️ Your model has a page on http://huggingface.co/models and everyone can load it using `AutoModel.from_pretrained(\"username/model_name\")`.\n",
        "\n",
        "[![tb](https://huggingface.co/blog/assets/01_how-to-train/model_page.png)](https://huggingface.co/julien-c/EsperBERTo-small)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8Q8Rhk9KBKi"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "# Part III: Topic Modelling using Laten Dirichelet Allocation\n",
        "\n",
        "In this final part, we will demonstrate how to perform Topic Modelling on a corpus of data.\n",
        "As a reminder, a \"corpus\" means a collection of text documents and can be of arbitrary size."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "t-5IicTuKBKi",
        "outputId": "297c7a5f-4385-45bc-add1-58916e6b9daa"
      },
      "source": [
        "!pip install gensim spacy pyLDAvis"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (3.6.0)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (2.2.4)\n",
            "Collecting pyLDAvis\n",
            "  Downloading pyLDAvis-3.3.1.tar.gz (1.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7 MB 5.4 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.4.1)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.19.5)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (5.2.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.6)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (57.4.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (7.4.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.6)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.6)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.8.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.62.3)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (4.8.2)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy) (3.10.0.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy) (3.6.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Collecting numpy>=1.11.3\n",
            "  Downloading numpy-1.21.4-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 15.7 MB 15.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (1.1.0)\n",
            "Collecting funcy\n",
            "  Downloading funcy-1.16-py2.py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (0.16.0)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (0.0)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (2.7.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (1.0.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (2.11.3)\n",
            "Collecting pandas>=1.2.0\n",
            "  Downloading pandas-1.3.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 11.3 MB 14.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.2.0->pyLDAvis) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.2.0->pyLDAvis) (2018.9)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->pyLDAvis) (2.0.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->pyLDAvis) (3.0.0)\n",
            "Building wheels for collected packages: pyLDAvis\n",
            "  Building wheel for pyLDAvis (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyLDAvis: filename=pyLDAvis-3.3.1-py2.py3-none-any.whl size=136897 sha256=b568e536f88848859fa152db9960949cc9cb1b35df0e620f04797c2871a6cd12\n",
            "  Stored in directory: /root/.cache/pip/wheels/c9/21/f6/17bcf2667e8a68532ba2fbf6d5c72fdf4c7f7d9abfa4852d2f\n",
            "Successfully built pyLDAvis\n",
            "Installing collected packages: numpy, pandas, funcy, pyLDAvis\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.19.5\n",
            "    Uninstalling numpy-1.19.5:\n",
            "      Successfully uninstalled numpy-1.19.5\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 1.1.5\n",
            "    Uninstalling pandas-1.1.5:\n",
            "      Successfully uninstalled pandas-1.1.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "yellowbrick 1.3.post1 requires numpy<1.20,>=1.16.0, but you have numpy 1.21.4 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas~=1.1.0; python_version >= \"3.0\", but you have pandas 1.3.4 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed funcy-1.16 numpy-1.21.4 pandas-1.3.4 pyLDAvis-3.3.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy",
                  "pandas"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gJ4RyUqBKBKi",
        "outputId": "114015c8-edf7-4eff-c76d-9f5f503205f6"
      },
      "source": [
        "import re, gensim, os, sys, spacy\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from gensim import models\n",
        "\n",
        "# Sklearn\n",
        "from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from pprint import pprint\n",
        "\n",
        "# Plotting tools\n",
        "import pyLDAvis\n",
        "import pyLDAvis.sklearn\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "print('Python Version: %s' % (sys.version))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python Version: 3.7.12 (default, Sep 10 2021, 00:21:48) \n",
            "[GCC 7.5.0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/past/types/oldstr.py:5: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
            "  from collections import Iterable\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 456
        },
        "id": "TJbt4JJxKBKj",
        "outputId": "96f6963c-a170-4f47-e836-efb420e95b39"
      },
      "source": [
        "dictionary = gensim.corpora.Dictionary.load('documents.dict')\n",
        "corpus = gensim.corpora.MmCorpus('documents.mm')\n",
        "lda_model = models.LdaModel.load('lda_model')\n",
        "ldamallet = models.wrappers.LdaMallet.load('ldamallet')\n",
        "optimal_model = models.wrappers.LdaMallet.load('optimal_model')\n",
        "\n",
        "print(dictionary)\n",
        "print(corpus)\n",
        "print(lda_model)\n",
        "print(ldamallet)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/smart_open/smart_open_lib.py:494: DeprecationWarning: This function is deprecated.  See https://github.com/RaRe-Technologies/smart_open/blob/develop/MIGRATING_FROM_OLDER_VERSIONS.rst for more information\n",
            "  warnings.warn(message, category=DeprecationWarning)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-00507e520ee6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdictionary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpora\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDictionary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'documents.dict'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mcorpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpora\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMmCorpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'documents.mm'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlda_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLdaModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'lda_model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mldamallet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrappers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLdaMallet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ldamallet'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0moptimal_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrappers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLdaMallet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'optimal_model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/utils.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, fname, mmap)\u001b[0m\n\u001b[1;32m    420\u001b[0m         \u001b[0mcompress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSaveLoad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_adapt_by_suffix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 422\u001b[0;31m         \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m         \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_specials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmmap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loaded %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/utils.py\u001b[0m in \u001b[0;36munpickle\u001b[0;34m(fname)\u001b[0m\n\u001b[1;32m   1356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1357\u001b[0m     \"\"\"\n\u001b[0;32m-> 1358\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0msmart_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1359\u001b[0m         \u001b[0;31m# Because of loading from S3 load can't be used (missing readline in smart_open)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1360\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36msmart_open\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    496\u001b[0m     \u001b[0mignore_ext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mignore_extension\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_extension\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 498\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mlocals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(uri, mode, buffering, encoding, errors, newline, closefd, opener, ignore_ext, compression, transport_params)\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnewline\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m     )\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36m_shortcut_open\u001b[0;34m(uri, mode, compression, buffering, encoding, errors, newline)\u001b[0m\n\u001b[1;32m    359\u001b[0m         \u001b[0mopen_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'errors'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 361\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_builtin_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffering\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mopen_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'documents.dict'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQvcdWldKBKj"
      },
      "source": [
        "import pickle\n",
        "#with open('documents', 'wb') as f: #save\n",
        "#    pickle.dump(mylist, f)\n",
        "\n",
        "with open('documents', 'rb') as f: #load\n",
        "    documents = pickle.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzcVsKOUKBKj"
      },
      "source": [
        "## Tokenize and Clean-up using gensim’s simple_preprocess()\n",
        "The sentences look better now, but you want to tokenize each sentence into a list of words, removing punctuations and unnecessary characters altogether.\n",
        "\n",
        "Gensim’s `simple_preprocess()` is great for this. Additionally I have set deacc=True to remove the punctuations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nmJ3MM6OKBKj"
      },
      "source": [
        "def sent_to_words(sentences):\n",
        "    for sentence in sentences:\n",
        "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
        "\n",
        "data_words = list(sent_to_words(documents))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wiLGp9CPKBKj"
      },
      "source": [
        "## Lemmatization\n",
        "Lemmatization is a process where we convert words to its root word.\n",
        "\n",
        "For example: ‘Studying’ becomes ‘Study’, ‘Meeting becomes ‘Meet’, ‘Better’ and ‘Best’ becomes ‘Good’.\n",
        "\n",
        "The advantage of this is, we get to reduce the total number of unique words in the dictionary. As a result, the number of columns in the document-word matrix (created by CountVectorizer in the next step) will be denser with lesser columns.\n",
        "\n",
        "You can expect better topics to be generated in the end."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LwMZm5RCKBKk"
      },
      "source": [
        "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
        "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
        "    texts_out = []\n",
        "    for sent in texts:\n",
        "        doc = nlp(\" \".join(sent)) \n",
        "        texts_out.append(\" \".join([token.lemma_ if token.lemma_ not in ['-PRON-'] else '' for token in doc if token.pos_ in allowed_postags]))\n",
        "    return texts_out\n",
        "\n",
        "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
        "# Run in terminal: python3 -m spacy download en\n",
        "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
        "\n",
        "# Do lemmatization keeping only Noun, Adj, Verb, Adverb\n",
        "data_lemmatized = lemmatization(documents, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vX60DCbIKBKk"
      },
      "source": [
        "## Create the Document-Word matrix\n",
        "The LDA topic model algorithm requires a document word matrix as the main input.\n",
        "\n",
        "You can create one using CountVectorizer. In the below code, I have configured the `CountVectorizer` to consider words that has occurred at least 10 times (min_df), remove built-in english stopwords, convert all words to lowercase, and a word can contain numbers and alphabets of at least length 3 in order to be qualified as a word.\n",
        "\n",
        "So, to create the doc-word matrix, you need to first initialise the CountVectorizer class with the required configuration and then apply fit_transform to actually create the matrix.\n",
        "\n",
        "Since most cells contain zeros, the result will be in the form of a sparse matrix to save memory.\n",
        "\n",
        "If you want to materialize it in a 2D array format, call the 1todense()1 method of the sparse matrix like its done in the next step."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5j6gq9KKBKk"
      },
      "source": [
        "vectorizer = CountVectorizer(analyzer='word',       \n",
        "                             min_df=4,                        # minimum reqd occurences of a word \n",
        "                             stop_words='english',             # remove stop words\n",
        "                             lowercase=True,                   # convert all words to lowercase\n",
        "                             token_pattern='[a-zA-Z0-9]{3,}',  # num chars > 3\n",
        "                             # max_features=50000,             # max number of uniq words\n",
        "                            )\n",
        "\n",
        "data_vectorized = vectorizer.fit_transform(data_lemmatized)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hz3EJ3XpKBKk"
      },
      "source": [
        "## Check the Sparsicity\n",
        "Sparsicity is nothing but the percentage of non-zero datapoints in the document-word matrix, that is data_vectorized.\n",
        "\n",
        "Since most cells in this matrix will be zero, I am interested in knowing what percentage of cells contain non-zero values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGwgTTOAKBKk"
      },
      "source": [
        "# Materialize the sparse data\n",
        "data_dense = data_vectorized.todense()\n",
        "\n",
        "# Compute Sparsicity = Percentage of Non-Zero cells\n",
        "print(\"Sparsicity: \", ((data_dense > 0).sum()/data_dense.size)*100, \"%\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6OQBLMzKBKk"
      },
      "source": [
        "## Build LDA model with sklearn\n",
        "Everything is ready to build a Latent Dirichlet Allocation (LDA) model. Let’s initialise one and call `fit_transform()` to build the LDA model.\n",
        "\n",
        "For this example, I have set the n_topics as 20 based on prior knowledge about the dataset. Later we will find the optimal number using grid search."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kKbfw4BKBKl"
      },
      "source": [
        "# Build LDA Model\n",
        "lda_model = LatentDirichletAllocation(n_components=20,               # Number of topics\n",
        "                                      max_iter=10,               # Max learning iterations\n",
        "                                      learning_method='online',   \n",
        "                                      random_state=100,          # Random state\n",
        "                                      batch_size=128,            # n docs in each learning iter\n",
        "                                      evaluate_every = -1,       # compute perplexity every n iters, default: Don't\n",
        "                                      n_jobs = -1,               # Use all available CPUs\n",
        "                                     )\n",
        "lda_output = lda_model.fit_transform(data_vectorized)\n",
        "\n",
        "print(lda_model)  # Model attributes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKXcatbDKBKl"
      },
      "source": [
        "## Diagnose model performance with perplexity and log-likelihood\n",
        "\n",
        "A model with higher log-likelihood and lower perplexity (exp(-1. * log-likelihood per word)) is considered to be good. Let’s check for our model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2r74mblKBKl"
      },
      "source": [
        "# Log Likelyhood: Higher the better\n",
        "print(\"Log Likelihood: \", lda_model.score(data_vectorized))\n",
        "\n",
        "# Perplexity: Lower the better. Perplexity = exp(-1. * log-likelihood per word)\n",
        "print(\"Perplexity: \", lda_model.perplexity(data_vectorized))\n",
        "\n",
        "# See model parameters\n",
        "print(lda_model.get_params())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBt2Dx-KKBKl"
      },
      "source": [
        "## How to GridSearch the best LDA model?\n",
        "The most important tuning parameter for LDA models is `n_components` (number of topics). In addition, I am going to search `learning_decay` (which controls the learning rate) as well.\n",
        "\n",
        "Besides these, other possible search params could be `learning_offset` (downweigh early iterations. Should be `> 1) and max_iter`. These could be worth experimenting if you have enough computing resources.\n",
        "\n",
        "Be warned, the grid search constructs multiple LDA models for all possible combinations of param values in the param_grid dict. So, this process can consume a lot of time and resources."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ROQNFs1yKBKl"
      },
      "source": [
        "# Define Search Param\n",
        "search_params = {'n_components': [10, 15, 20, 25, 30], 'learning_decay': [.5, .7, .9]}\n",
        "\n",
        "# Init the Model\n",
        "lda = LatentDirichletAllocation()\n",
        "\n",
        "# Init Grid Search Class\n",
        "model = GridSearchCV(lda, param_grid=search_params)\n",
        "\n",
        "# Do the Grid Search\n",
        "model.fit(data_vectorized)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCeIo4nKKBKl"
      },
      "source": [
        "## How to see the best topic model and its parameters?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pdl1MUSNKBKm"
      },
      "source": [
        "# Best Model\n",
        "best_lda_model = model.best_estimator_\n",
        "\n",
        "# Model Parameters\n",
        "print(\"Best Model's Params: \", model.best_params_)\n",
        "\n",
        "# Log Likelihood Score\n",
        "print(\"Best Log Likelihood Score: \", model.best_score_)\n",
        "\n",
        "# Perplexity\n",
        "print(\"Model Perplexity: \", best_lda_model.perplexity(data_vectorized))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P72-Fu5sKBKm"
      },
      "source": [
        "# Get Log Likelyhoods from Grid Search Output\n",
        "log_likelyhoods_5 = []\n",
        "log_likelyhoods_7 = []\n",
        "log_likelyhoods_9 = []\n",
        "\n",
        "for i in range(len(model.cv_results_['params'])):\n",
        "    if model.cv_results_['params'][i]['learning_decay'] == 0.5:\n",
        "       log_likelyhoods_5.append(round(model.cv_results_['mean_test_score'][i]))\n",
        "    elif model.cv_results_['params'][i]['learning_decay'] == 0.7:\n",
        "       log_likelyhoods_7.append(round(model.cv_results_['mean_test_score'][i]))\n",
        "    elif model.cv_results_['params'][i]['learning_decay'] == 0.9:\n",
        "       log_likelyhoods_9.append(round(model.cv_results_['mean_test_score'][i]))\n",
        "\n",
        "# Show graph\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.plot(n_topics, log_likelyhoods_5, label='0.5')\n",
        "plt.plot(n_topics, log_likelyhoods_7, label='0.7')\n",
        "plt.plot(n_topics, log_likelyhoods_9, label='0.9')\n",
        "plt.title(\"Choosing Optimal LDA Model\")\n",
        "plt.xlabel(\"Num Topics\")\n",
        "plt.ylabel(\"Log Likelyhood Scores\")\n",
        "plt.legend(title='Learning decay', loc='best')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pyQc_OW_KBKm"
      },
      "source": [
        "## How to see the dominant topic in each document?\n",
        "To classify a document as belonging to a particular topic, a logical approach is to see which topic has the highest contribution to that document and assign it.\n",
        "\n",
        "In the table below, I’ve greened out all major topics in a document and assigned the most dominant topic in its own column."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SDjBq94cKBKm"
      },
      "source": [
        "# Create Document - Topic Matrix\n",
        "lda_output = best_lda_model.transform(data_vectorized)\n",
        "\n",
        "# column names\n",
        "topicnames = [\"Topic\" + str(i) for i in range(best_lda_model.n_components)]\n",
        "\n",
        "# index names\n",
        "docnames = [\"Doc\" + str(i) for i in range(len(documents))]\n",
        "\n",
        "# Make the pandas dataframe\n",
        "df_document_topic = pd.DataFrame(np.round(lda_output, 2), columns=topicnames, index=docnames)\n",
        "\n",
        "# Get dominant topic for each document\n",
        "dominant_topic = np.argmax(df_document_topic.values, axis=1)\n",
        "df_document_topic['dominant_topic'] = dominant_topic\n",
        "\n",
        "# Styling\n",
        "def color_green(val):\n",
        "    color = 'green' if val > .1 else 'black'\n",
        "    return 'color: {col}'.format(col=color)\n",
        "\n",
        "def make_bold(val):\n",
        "    weight = 700 if val > .1 else 400\n",
        "    return 'font-weight: {weight}'.format(weight=weight)\n",
        "\n",
        "# Apply Style\n",
        "df_document_topics = df_document_topic.head(15).style.applymap(color_green).applymap(make_bold)\n",
        "df_document_topics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFudPfNmKBKm"
      },
      "source": [
        "## Review topics distribution across documents"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJAqLIjfKBKm"
      },
      "source": [
        "df_topic_distribution = df_document_topic['dominant_topic'].value_counts().reset_index(name=\"Num Documents\")\n",
        "df_topic_distribution.columns = ['Topic Num', 'Num Documents']\n",
        "df_topic_distribution"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEXlR0p2KBKn"
      },
      "source": [
        "## How to visualize the LDA model with pyLDAvis?\n",
        "The pyLDAvis offers the best visualization to view the topics-keywords distribution."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-bKbEZcTKBKn"
      },
      "source": [
        "pyLDAvis.enable_notebook()\n",
        "panel = pyLDAvis.sklearn.prepare(best_lda_model, data_vectorized, vectorizer, mds='tsne')\n",
        "panel"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39sJ2QTbKBKn"
      },
      "source": [
        "## How to see the Topic’s keywords?\n",
        "The weights of each keyword in each topic is contained in `lda_model.components_` as a 2d array. The names of the keywords itself can be obtained from vectorizer object using `get_feature_names()`.\n",
        "\n",
        "Let’s use this info to construct a weight matrix for all keywords in each topic."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xjy23ygZKBKn"
      },
      "source": [
        "# Topic-Keyword Matrix\n",
        "df_topic_keywords = pd.DataFrame(best_lda_model.components_)\n",
        "\n",
        "# Assign Column and Index\n",
        "df_topic_keywords.columns = vectorizer.get_feature_names()\n",
        "df_topic_keywords.index = topicnames\n",
        "\n",
        "# View\n",
        "df_topic_keywords.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AvIgN4TGKBKn"
      },
      "source": [
        "## Get the top 15 keywords each topic\n",
        "From the above output, I want to see the top 15 keywords that are representative of the topic.\n",
        "\n",
        "The `show_topics()` defined below creates that."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SfnFa5LcKBKn"
      },
      "source": [
        "# Show top n keywords for each topic\n",
        "def show_topics(vectorizer=vectorizer, lda_model=lda_model, n_words=20):\n",
        "    keywords = np.array(vectorizer.get_feature_names())\n",
        "    topic_keywords = []\n",
        "    for topic_weights in lda_model.components_:\n",
        "        top_keyword_locs = (-topic_weights).argsort()[:n_words]\n",
        "        topic_keywords.append(keywords.take(top_keyword_locs))\n",
        "    return topic_keywords\n",
        "\n",
        "topic_keywords = show_topics(vectorizer=vectorizer, lda_model=best_lda_model, n_words=15)        \n",
        "\n",
        "# Topic - Keywords Dataframe\n",
        "df_topic_keywords = pd.DataFrame(topic_keywords)\n",
        "df_topic_keywords.columns = ['Word '+str(i) for i in range(df_topic_keywords.shape[1])]\n",
        "df_topic_keywords.index = ['Topic '+str(i) for i in range(df_topic_keywords.shape[0])]\n",
        "df_topic_keywords"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rvv9GGiLKBKo"
      },
      "source": [
        "## How to predict the topics for a new piece of text?\n",
        "Assuming that you have already built the topic model, you need to take the text through the same routine of transformations and before predicting the topic.\n",
        "\n",
        "For our case, the order of transformations is:\n",
        "\n",
        "`sent_to_words() –> lemmatization() –> vectorizer.transform() –> best_lda_model.transform()`\n",
        "\n",
        "You need to apply these transformations in the same order. So to simplify it, let’s combine these steps into a `predict_topic()` function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 486
        },
        "id": "uLxh8pRgKBKo",
        "outputId": "44352e19-da28-4092-e4ce-c64e27fef956"
      },
      "source": [
        "# Define function to predict topic for a given text document.\n",
        "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
        "\n",
        "def predict_topic(text, nlp=nlp):\n",
        "    global sent_to_words\n",
        "    global lemmatization\n",
        "\n",
        "    # Step 1: Clean with simple_preprocess\n",
        "    mytext_2 = list(sent_to_words(text))\n",
        "\n",
        "    # Step 2: Lemmatize\n",
        "    mytext_3 = lemmatization(mytext_2, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
        "\n",
        "    # Step 3: Vectorize transform\n",
        "    mytext_4 = vectorizer.transform(mytext_3)\n",
        "\n",
        "    # Step 4: LDA Transform\n",
        "    topic_probability_scores = best_lda_model.transform(mytext_4)\n",
        "    topic = df_topic_keywords.iloc[np.argmax(topic_probability_scores), :].values.tolist()\n",
        "    return topic, topic_probability_scores\n",
        "\n",
        "# Predict the topic\n",
        "mytext = [\"Some text about christianity and bible\"]\n",
        "topic, prob_scores = predict_topic(text = mytext)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/catalogue.py:138: DeprecationWarning: SelectableGroups dict interface is deprecated. Use select.\n",
            "  for entry_point in AVAILABLE_ENTRY_POINTS.get(self.entry_point_namespace, []):\n",
            "/usr/local/lib/python3.7/dist-packages/catalogue.py:138: DeprecationWarning: SelectableGroups dict interface is deprecated. Use select.\n",
            "  for entry_point in AVAILABLE_ENTRY_POINTS.get(self.entry_point_namespace, []):\n",
            "/usr/local/lib/python3.7/dist-packages/catalogue.py:126: DeprecationWarning: SelectableGroups dict interface is deprecated. Use select.\n",
            "  for entry_point in AVAILABLE_ENTRY_POINTS.get(self.entry_point_namespace, []):\n",
            "/usr/local/lib/python3.7/dist-packages/catalogue.py:138: DeprecationWarning: SelectableGroups dict interface is deprecated. Use select.\n",
            "  for entry_point in AVAILABLE_ENTRY_POINTS.get(self.entry_point_namespace, []):\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-1fdc8751dd21>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# Predict the topic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mmytext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"Some text about christianity and bible\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mtopic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprob_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_topic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmytext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-4-1fdc8751dd21>\u001b[0m in \u001b[0;36mpredict_topic\u001b[0;34m(text, nlp)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# Step 1: Clean with simple_preprocess\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mmytext_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent_to_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# Step 2: Lemmatize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'sent_to_words' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybwgbczVKBKo"
      },
      "source": [
        "## How to cluster documents that share similar topics and plot?\n",
        "You can use k-means clustering on the document-topic probabilioty matrix, which is nothing but lda_output object. Since out best model has 4 clusters, I’ve set `n_clusters=4` `in KMeans()`.\n",
        "\n",
        "Alternately, you could avoid k-means and instead, assign the cluster as the topic column number with the highest probability score.\n",
        "\n",
        "We now have the cluster number. But we also need the X and Y columns to draw the plot.\n",
        "\n",
        "For the X and Y, you can use SVD on the `lda_output` object with `n_components` as 2. SVD ensures that these two columns captures the maximum possible amount of information from `lda_output` in the first 2 components."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFM58u5eKBKo"
      },
      "source": [
        "# Construct the k-means clusters\n",
        "from sklearn.cluster import KMeans\n",
        "clusters = KMeans(n_clusters=4, random_state=100).fit_predict(lda_output)\n",
        "\n",
        "# Build the Singular Value Decomposition(SVD) model\n",
        "svd_model = TruncatedSVD(n_components=2)  # 2 components\n",
        "lda_output_svd = svd_model.fit_transform(lda_output)\n",
        "\n",
        "# X and Y axes of the plot using SVD decomposition\n",
        "x = lda_output_svd[:, 0]\n",
        "y = lda_output_svd[:, 1]\n",
        "\n",
        "# Weights for the 15 columns of lda_output, for each component\n",
        "print(\"Component's weights: \\n\", np.round(svd_model.components_, 2))\n",
        "\n",
        "# Percentage of total information in 'lda_output' explained by the two components\n",
        "print(\"Perc of Variance Explained: \\n\", np.round(svd_model.explained_variance_ratio_, 2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0PSIfHHKBKo"
      },
      "source": [
        "We have the X, Y and the cluster number for each document.\n",
        "\n",
        "Let’s plot the document along the two SVD decomposed components. The color of points represents the cluster number (in this case) or topic number."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W9apQuuLKBKo"
      },
      "source": [
        "# Plot\n",
        "plt.figure(figsize=(12, 12))\n",
        "plt.scatter(x, y, c=clusters)\n",
        "plt.xlabel('Component 2')\n",
        "plt.xlabel('Component 1')\n",
        "plt.title(\"Segregation of Topic Clusters\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddrASmyaKBKo"
      },
      "source": [
        "## How to get similar documents for any given piece of text?\n",
        "Once you know the probaility of topics for a given document (using `predict_topic()`), compute the euclidean distance with the probability scores of all other documents.\n",
        "\n",
        "The most similar documents are the ones with the smallest distance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jtqr6Z0tKBKp"
      },
      "source": [
        "from sklearn.metrics.pairwise import euclidean_distances\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
        "\n",
        "def similar_documents(text, doc_topic_probs, documents = documents, nlp=nlp, top_n=5, verbose=False):\n",
        "    topic, x  = predict_topic(text)\n",
        "    dists = euclidean_distances(x.reshape(1, -1), doc_topic_probs)[0]\n",
        "    doc_ids = np.argsort(dists)[:top_n]\n",
        "    if verbose:        \n",
        "        print(\"Topic KeyWords: \", topic)\n",
        "        print(\"Topic Prob Scores of text: \", np.round(x, 1))\n",
        "        print(\"Most Similar Doc's Probs:  \", np.round(doc_topic_probs[doc_ids], 1))\n",
        "    return doc_ids, np.take(documents, doc_ids)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-XqSOUTKBKp"
      },
      "source": [
        "# Get similar documents\n",
        "mytext = [\"Some text about christianity and bible\"]\n",
        "doc_ids, docs = similar_documents(text=mytext, doc_topic_probs=lda_output, documents = documents, top_n=1, verbose=True)\n",
        "print('\\n', docs[0][:10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbgrN0_AKBKp"
      },
      "source": [
        "---\n",
        "\n",
        "# Takeaway Messages:\n",
        "\n",
        "- NLP is a **subfield of machine learning** concerned with the application of **learning algorthms to text and speech**.\n",
        "- NLP-based methods are typically applicable to all sequential-type information (*e.g.* DNA sequences, audio signals, time-series signals, *etc.*) however, they are predominantly used in human language applications.\n",
        "- For example, we can use NLP to create systems including:\n",
        "    1. **speech recognition** (*e.g.* real-time captioning)\n",
        "    2. **document summarization**\n",
        "    3. **machine translation**\n",
        "    4. **spam detection**\n",
        "    5. **named entity recognition**\n",
        "    6. **question answering**\n",
        "    7. **autocomplete** (*i.e.* predictive typing)\n",
        "- **Sentence Tokenization:** Sentence tokenization (also called **sentence segmentation**) is the problem of *dividing a string of written language into its component sentences*.\n",
        "- **Word Tokenization**: Word tokenization (also called **word segmentation**) is the problem of *dividing a string of written language into its component words*.\n",
        "- **Text Lemmatization & Stemming**: The goal of both stemming (crude) and lemmatization (refiined) is to *reduce inflectional forms* and sometimes derivationally related forms of a word to a common base form. For example, \"drive\" & \"drives\" & \"driving\" all have the same semantic meaning and should be combined.\n",
        "- **Stop Words**: Stop words usually refer to the **most common words** such as “and”, “the”, “a” in a language and when applying machine learning to text, **these words can add a lot of noise** so we remove them.\n",
        "- **Regex**: A regular expression, regex, or regexp is a sequence of characters that define *a search pattern to apply additional filtering* to our text. For example, we can remove all the non-words characters. In many cases, we don’t need the punctuation marks and it’s easy to remove them with regex.\n",
        "- **Bag-of-Words**: Machine learning algorithms *cannot work with raw text directly*, we need to convert the text into vectors of numbers (i.e. feature extraction) and the *bag-of-words mode*l is a popular and simple feature extraction technique that *counts the occurrence of each word within a document*.\n",
        "- **TF-IDF**: One problem with scoring word frequency is that *the most frequent words in the document start to have the highest scores* (frequent words may not havee much “informational gain”) to the model so we penalize words that are frequent across all the documents using TF-IDF (**term frequency-inverse document frequency** is a s**tatistical measure** used to evaluate the **importance of a word** to a **document in a collection or corpus**)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Dqe6SOGKBKp"
      },
      "source": [
        "# FIN"
      ]
    }
  ]
}